{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Preparing Data for CycleLighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format\n",
    "\n",
    "Data used for training can be completely unpaired or paired. Unpaired datasets can be of different sizes and domains. Validation and training data must be labelled examples to calculate some metric of error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data & Preparation\n",
    "\n",
    "Import training and validation data in from whatever data source you have. In this case I am reading raw text for the unpaired examples and a CSV for examples that would have been manually annotated or taken from a ground truth data source. Split your unpaired content in the appropriate way e.g. into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 sentences for dataset A\n",
      "120 sentences for dataset B\n",
      "4 sentences for validation\n"
     ]
    }
   ],
   "source": [
    "# import text from file\n",
    "with open('a_text.txt', 'r') as f:\n",
    "    a_text = f.read()\n",
    "\n",
    "with open('b_text.txt', 'r') as f:\n",
    "    b_text = f.read()\n",
    "\n",
    "val_data = pd.read_csv('val_data.csv').to_dict('records')\n",
    "\n",
    "# Extract each sentence\n",
    "a_sentences = sent_tokenize(a_text)\n",
    "b_sentences = sent_tokenize(b_text)\n",
    "\n",
    "print(f'{len(a_sentences)} sentences for dataset A')\n",
    "print(f'{len(b_sentences)} sentences for dataset B')\n",
    "print(f'{len(val_data)} sentences for validation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with `text` as the key for the unpaired data and `text` and `label` as the keys for the paired data. The values for each key should be a list of strings.\n",
    "\n",
    "Validation samples can be reused for each dataset by inverting which half of the pair is text and which half is label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.'}\n"
     ]
    }
   ],
   "source": [
    "a_dict = [{'text': a.strip()} for a in a_sentences]\n",
    "b_dict = [{'text': b.strip()} for b in b_sentences]\n",
    "a_val = [{'text': example['A'], 'label': example['B']} for example in val_data]\n",
    "b_val = [{'text': example['B'], 'label': example['A']} for example in val_data]\n",
    "\n",
    "print(a_dict[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using JSON and JSONL\n",
    "\n",
    "JSON & JSONL samples cannot be used with a validation set currently. Test samples may be generated for use in `generate.py` so we will keep the full validation set for testing. A and B datasets do not need to be the same. We will save one as JSON and one as JSONL as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "with open('a.json', 'w+') as f:\n",
    "    json.dump(a_dict, f, indent=4)\n",
    "\n",
    "# JSONL\n",
    "with open('b.jsonl', 'w+') as f:\n",
    "    for b in b_dict:\n",
    "        f.write(json.dumps(b))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetDict\n",
    "\n",
    "The `DatasetDict` object is a dictionary that holds multiple datasets. It is used to hold the training, validation, and test datasets. One again, each dataset could be different types e.g. one could be JSON and the other DatasetDict. We will create both as DatasetDict as it is the recommended format for the project in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dataset = Dataset.from_list(a_dict)\n",
    "b_dataset = Dataset.from_list(b_dict)\n",
    "\n",
    "a_val_dataset = Dataset.from_list(a_val)\n",
    "b_val_dataset = Dataset.from_list(b_val)\n",
    "\n",
    "# Split out validation and test data with 50% for val and 50% for test\n",
    "a_val_dataset, a_test_dataset = a_val_dataset.train_test_split(test_size=0.5).values()\n",
    "b_val_dataset, b_test_dataset = b_val_dataset.train_test_split(test_size=0.5).values()\n",
    "\n",
    "a_dataset_dict = DatasetDict({\n",
    "    'train': a_dataset,\n",
    "    'validation': a_val_dataset,\n",
    "    'test': a_test_dataset\n",
    "})\n",
    "\n",
    "b_dataset_dict = DatasetDict({\n",
    "    'train': b_dataset,\n",
    "    'validation': b_val_dataset,\n",
    "    'test': b_test_dataset\n",
    "})\n",
    "\n",
    "a_dataset_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save datasets to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    }
   ],
   "source": [
    "a_dataset_dict.save_to_disk('tutorial/A')\n",
    "b_dataset_dict.save_to_disk('tutorial/B')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
